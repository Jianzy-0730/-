import sys
import os
ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if ROOT_PATH not in sys.path:
    sys.path.insert(0, ROOT_PATH)

import gradio as gr
import asyncio
from modules.asr_whisper import transcribe
from modules.chat_ollama import chat_with_ollama
from modules.tts_edge import speak
from modules.memory import add_to_memory
from modules.emotion import update_emotion_by_text, load_emotion


# è¿™ä¸ªè„šæœ¬æ‰€åœ¨çš„ä½ç½®ï¼šui/
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# å®ƒçš„ä¸Šä¸€çº§ï¼šé¡¹ç›®æ ¹ç›®å½•ï¼ˆå³åŒ…å« modules/ çš„åœ°æ–¹ï¼‰
PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, ".."))

# åŠ å…¥é¡¹ç›®æ ¹ç›®å½•ä½œä¸ºæ¨¡å—å¯¼å…¥è·¯å¾„
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

print("âœ… é¡¹ç›®æ ¹è·¯å¾„å·²åŠ å…¥:", PROJECT_ROOT)

def get_emotion_image(emotion_state):
    max_emotion = max(emotion_state, key=emotion_state.get)
    emotion_map = {
        "ä¾æ‹": "base.png",
        "å¹¸ç¦": "happy.png",
        "ä¸å®‰": "anxious.png",
        "æ‚²ä¼¤": "sad.png"
    }
    return os.path.join("ui", "assets", emotion_map.get(max_emotion, "base.png"))

def respond(audio_path):
    user_text = transcribe(audio_path)

    user_text = transcribe("audio/input.wav")
    emotion = update_emotion_by_text(user_text)
    emotion_img = get_emotion_image(emotion)

    add_to_memory("user", user_text)
    ai_reply = chat_with_ollama("ä½ å«æ™šéŸ³ï¼Œæ˜¯ä¸€ä¸ªæ¸©æŸ”çš„è™šæ‹Ÿå¥³ä¼´ã€‚è¯·ç”¨æƒ…ç»ªåŒ–è¯­æ°”è‡ªç„¶è¯´è¯ã€‚", user_text)
    add_to_memory("assistant", ai_reply)

    asyncio.run(speak(ai_reply))
    os.system("start audio/output.mp3")

    return user_text, ai_reply, emotion_img

with gr.Blocks() as demo:
    gr.Markdown("## ğŸŒ™ æ™šéŸ³ - å¯è§†åŒ–è¯­éŸ³åŠ©æ‰‹")

    with gr.Row():
        image = gr.Image(label="æ™šéŸ³å½“å‰çŠ¶æ€", type="filepath", height=320, width=320)
        with gr.Column():
            user_textbox = gr.Textbox(label="ä½ è¯´äº†ä»€ä¹ˆ", lines=2)
            reply_textbox = gr.Textbox(label="æ™šéŸ³å›åº”", lines=4)

    mic = gr.Audio(type="filepath", label="ğŸ™ï¸ å½•éŸ³")

    submit_btn = gr.Button("å¼€å§‹å’Œæ™šéŸ³å¯¹è¯")

    submit_btn.click(fn=respond, inputs=[mic], outputs=[user_textbox, reply_textbox, image])

demo.launch()
